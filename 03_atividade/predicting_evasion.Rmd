---
title: "Predicting Evasion"
author: "Caynan Sousa"
date: "26/2/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# For data manipulation and tidying
library(dplyr)
library(tidyr)
library(Hmisc)
library(reshape2)

# For data visualizations
library(ggplot2)
library(highcharter)
library(igraph)
library(knitr)
library(htmlwidgets)
library(gridExtra)

# For machine learning
library(caret)
library(glmnet)
library(C50)
library(mlbench)
library(ranger)
library(e1071)
```

## Introduction


### Importing Dataset

```{r load_data}
# Load data and set it to `data`
data.url <- "https://raw.githubusercontent.com/caynan/data_analysis_2/master/03_atividade/treino_classificacao_v2.csv"
data <- read.csv(data.url)

# rename columns
colnames(data) <- c("student_id", "course_id", "course_name", "year", "term", "mean", "evaded")
```


### Data Structure and Variables

Great! Now that our data is imported, we can get a look at it. I'll start by looking at it's structure.

```{r data_structure}
str(data)
```

So as we expected we're working with a data frame of 7 variables and 7874 observations.

Our 7 variables are:   

- **student_id**: An anonimized string that uniquely identify students
- **course_id**: The id of the course (duh)
- **course_name**: The Course name (duh)
- **year**: The year on which the record is from
- **term**: The term (1st or 2nd semester) on which the record is from
- **mean**: The grade the student received at the end of the course
- **evaded**: If the student evaded

I'm going to change the data type of the student_id, course_id, term and year columns. Apart from year, which is ordinal, all the other should be factors (nominal). 

```{r change_datatypes}
# change data types for student_id, course_id, evaded, year and term
data$student_id <- as.factor(data$student_id)
data$course_id <- as.factor(data$course_id)
data$term <- as.factor(data$term)
data$year <- as.ordered(data$year)
```

A few basic questions that we might have about our data is:

How many years are we looking here?
```{r how_many_years}
levels(data$year)
```

Cool, so we're dealing with data since the year 2000 up to 2015.

How many students data do we have in our data?
```{r how_many_students}
data %>% summarise(n_distinct(student_id))
```
only **1351** students were accepted into Computer Science in the past 15 years, wow!

## Can we predict if a student will evade after the first semester?
bla bla bla, reasons why this is important

### Feature Engineering

I'm going to use some simple machine learning models (logistic regression and decision trees) to see if I can predict if a student will evade or not after the first
semester. Summarising, we're trying to predict if a student evaded or not (dependent variable)  and for that we're going to use 10 (+ 1 extra) independent variables:  

 - Grade on Calculus I (aka: calculus)
 - Grade on Vectorial Algebra (aka: vectorial)
 - Grade on Intro to CS (aka: ICS)
 - Grade on Programming I (aka: P1)
 - Grade on Lab. of Programming I (aka: LP1)
 - Grade on Reading and Producing Text (aka: RPT)
 - General GPA
 - Comp. Sci. GPA (taking in consideration only classes from the Systems and Computers Department)
 - Math GPA (taking in consideration only classes from the Math and Statistics Department)
 - Term (if they got accepted into the first or the second semester)

But first, we need to transform our dataset to fit into a frame with the needed variables.

```{r reshaping_data}
# reshape data
# - only one entry per student_id
# - reshape course mean into column

# get only one entry per student
unique.data <- data %>%
  group_by(student_id) %>% select(student_id, evaded, year, term) %>% unique()

# reshape courses into columns
data.reshaped.by.course <- data %>%
  select(student_id, course_name, mean) %>%
  mutate(course_name = as.factor(gsub(" ", ".", course_name))) %>%
  dcast(student_id ~ course_name, value.var = "mean")
  
# merge the two DFs
mlset <- merge(unique.data, data.reshaped.by.course)
# set column names to smaller easier to type names, why you wonder, because I'm lazy ;p
colnames(mlset) <- c("student_id", "evaded","year", "term", "vectorial", "calculus", "ICS", "LP1", "RPT", "P1")
```

As we can see, we have a lot of NA on the courses columns, which mean that those students don't have record for those courses. This phenomenon could be explained by a number of reasons, could be that the student coursed them elsewhere, or it could be that he failed by the number of absences.

Since is a little bit tricky the reason on why a given student don't have records for a given class, we're going to solve that in two steps:  

- First we need to keep track of how many missing records(NA) we have per student (row)
- Second, let's introduce a simple algorithm to fill those columns, if the number of missing records is greater than 3 (half the number of mandatory classes), let's set the mean to zero (0.0), But if is less than or equal to 3 let's set the value to the 

mean of the respective course for all other students on that given year and term.

```{r dealing_with_na}
# count the number of missing records
mlset$missing_records <- apply(is.na(mlset), 1, sum)

# implement our algorithm for each one of the courses column
mlset <- mlset %>%
  group_by(year, term) %>%
  mutate(calculus = ifelse(is.na(calculus), ifelse(missing_records > 3, 0, median(calculus, na.rm = TRUE)), calculus)) %>%
  mutate(vectorial = ifelse(is.na(vectorial), ifelse(missing_records > 3, 0, median(vectorial, na.rm = TRUE)), vectorial)) %>%
  mutate(ICS = ifelse(is.na(ICS), ifelse(missing_records > 3, 0, median(ICS, na.rm = TRUE)), ICS)) %>%
  mutate(P1 = ifelse(is.na(P1), ifelse(missing_records > 3, 0, median(P1, na.rm = TRUE)), P1)) %>%
  mutate(LP1 = ifelse(is.na(LP1), ifelse(missing_records > 3, 0, median(LP1, na.rm = TRUE)), LP1)) %>%
  mutate(RPT = ifelse(is.na(RPT), ifelse(missing_records > 3, 0, median(RPT, na.rm = TRUE)), RPT))

# We don't have any use for the year column, so let's drop it.
 mlset$year <- NULL
```

Now let's add our GPA variables

```{r calculated_variables}
# General GPA
gen.gpa <- mlset %>% rowwise() %>%
  mutate(gen_gpa = sum(vectorial, calculus, ICS, LP1, RPT, P1) / 6)
# CompSci GPA
compsci.gpa <- mlset %>% rowwise() %>%
  mutate(compsci_gpa = sum(ICS, LP1, P1) / 3)
# Math GPA
math.gpa <- mlset %>% rowwise() %>%
  mutate(math_gpa = sum(vectorial, calculus) / 2)

mlset$general_gpa <- gen.gpa$gen_gpa
mlset$compsci_gpa <- compsci.gpa$compsci_gpa
mlset$math_gpa <- math.gpa$math_gpa
```

### Is our Data Unbalanced?

Let's take a look if our data is unbalanced, have way more students in one category than on another (way more students who didn't evaded than students who evaded).

```{r more_dropouts, message=FALSE, warning=FALSE}
num_dropouts <- mlset %>%
  dplyr::summarise(num_students = n(),
            num_dropouts = sum(evaded)) %>%
  ungroup() %>%
  dplyr::summarise(total_students = sum(num_students),
            total_dropouts = sum(num_dropouts))

total_dropouts.melt <- num_dropouts %>% 
  melt(variable.name="data_class", value.name="total_count")
```

We can plot this information to make it easier to see our distribution.

```{r num_dropouts_plot, echo=FALSE}
ggplot(total_dropouts.melt, aes(data_class, total_count)) +
  geom_bar(stat = "identity", position = "dodge", fill="#56B4E9") +
  geom_text(aes(label=sprintf("%d", total_count)), size = 3) +
  # scale_x_continuous(breaks=seq(2000, 2015, 1)) +
  ylab("Number of Students") +
  xlab("Data Classes")
```

As we can see our data is unbalanced, we have way more students that who didn't evaded than ones that have. To be more precise, we have that only ~9.32% of the data is of students that evaded.
But don't worry we will deal with this in a bit, but first let's see how our prediction models work with this unbalanced data, so we can have a baseline to compare with.


### Modeling

Now that the dataframe is set up the way I want (a single row per student and only the variables I'm interested in), I'm going to randomize the dataset and extract a test set.

```{r create_testing_set}
# Set a random seed
set.seed(42)

# Shuffle rows of the data set
n <- nrow(mlset)
shuffled <- mlset[sample(n), ]

# Perform 80/20 split (80% to training set, 30% to test set)
train_indices <- 1:round(0.8 * n)
train <- shuffled[train_indices, ]

test_indices <- (round(0.8 * n) + 1):n
test <- shuffled[test_indices, ]
```


#### Fitting a baseline model
probably something along the line of predicting that the student will not evade (which is good because of the unbalance)


```{r control}
myControl <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
```

Fitting a Logistic Regression model, and then let's try to optimize it by using Lasso and Ridge.

```{r fitting_model, message=FALSE, warning=FALSE, fig.align='center'}
# non-optmized logistic regression model
reg_model <- train(as.factor(evaded) ~ term + vectorial + calculus + ICS + LP1 + RPT + P1 + missing_records + general_gpa + compsci_gpa + math_gpa,
                   data = train, tuneLength = 3, method = "glm", family = "binomial", na.action = na.omit)

# predicting test data
predict_test_reg_model <- predict(reg_model, test)
cm_test_reg_model <- confusionMatrix(predict_test_reg_model, test$evaded)

# predicting train data
predict_train_reg_model <- predict(reg_model, train)
cm_train_reg_model <- confusionMatrix(predict_train_reg_model, train$evaded)

reg_model.plot <- ggplot(varImp(reg_model), top = 5[1]) + ggtitle("Most Important Variables\nReg. Model") +
  theme(plot.title = element_text(hjust = 0.5))

# Optimize feature selection for model
reg_model_opt <- train(as.factor(evaded) ~ term + vectorial + calculus + ICS + LP1 + RPT + P1 + missing_records + general_gpa + compsci_gpa + math_gpa,
               data = train, tuneLength = 3, method="glmnet", family = "binomial", trControl = myControl, na.action = na.omit)

# predicting test data
predict_test_reg_model_opt <- predict(reg_model_opt, test)
cm_test_reg_model_opt <- confusionMatrix(predict_test_reg_model_opt, test$evaded)
# predicting train data
predict_train_reg_model_opt <- predict(reg_model_opt, train)
cm_train_reg_model_opt <- confusionMatrix(predict_train_reg_model_opt, train$evaded)

reg_model_opt.plot <- ggplot(varImp(reg_model_opt), top = 5[1]) + ggtitle("Most Important Variables\nOpt. Reg. Model") +
  theme(plot.title = element_text(hjust = 0.5))

grid.arrange(reg_model.plot ,reg_model_opt.plot, ncol=2)

```


#### Boosted C5.0
C5.0 is a 'better' version of C4.5 which is a better instance of the ID3 algorithm for creating decision trees.
So let's play with it and see if we can see better results from a decision tree model.


```{r c5_0}
C50_model <- train(as.factor(evaded) ~ term + vectorial + calculus + ICS + LP1 + RPT + P1 + missing_records + general_gpa + compsci_gpa + math_gpa,
               data = train, tuneLength = 3, method="C5.0Rules", family = "binomial", metric="Kappa")
# predicting test data
predict_test_C50_model <- predict(C50_model, test)
cm_test_C50_model <- confusionMatrix(predict_test_C50_model, test$evaded)
# predicting train data
predict_train_C50_model <- predict(C50_model, train)
cm_train_C50_model <- confusionMatrix(predict_train_C50_model, train$evaded)

# Ploting the most important variables
C50_model.plot <- ggplot(varImp(C50_model), top = 5[1]) + ggtitle("Most Important Variables\nC5.0 Model") +
  theme(plot.title = element_text(hjust = 0.5))

C50_model_opt <- train(as.factor(evaded) ~ term + vectorial + calculus + ICS + LP1 + RPT + P1 + missing_records + general_gpa + compsci_gpa + math_gpa,
               data = train, tuneLength = 3, method="C5.0Rules", family = "binomial", trControl = myControl, earlyStopping = TRUE, metric="Kappa")

# predicting test data
predict_test_C50_model_opt <- predict(C50_model_opt, test)
cm_test_C50_model_opt <- confusionMatrix(predict_test_C50_model_opt, test$evaded)
# predicting train data
predict_train_C50_model_opt <- predict(C50_model_opt, train)
cm_train_C50_model_opt <- confusionMatrix(predict_train_C50_model_opt, train$evaded)

C50_model_opt.plot <- ggplot(varImp(C50_model_opt), top = 5[1]) + ggtitle("Most Important Variables\nOpt. C5.0 Model") +
  theme(plot.title = element_text(hjust = 0.5))

grid.arrange(C50_model.plot ,C50_model_opt.plot, ncol=2)
```




#### Comparing models

Let's compare our models, and see if we can identify which model explained better our data variation.
```{r compare_models}
# Create list of models
opt_models <- list(reg_opt = reg_model_opt, C50_opt=C50_model_opt)
# Resample the models
opt_resampled <- resamples(opt_models)
# Generate a summary
summary(opt_resampled)

# Create list of models
models <- list(reg = reg_model, C50=C50_model)
# Resample the models
resampled <- resamples(models)
# Generate a summary
summary(resampled)
```

As we can see, our regression without any optimization is the one that modeled our data the best, I'm using the [Kappa](https://en.wikipedia.org/wiki/Cohen's_kappa) measure to affirm this,
given that our data is unbalanced Kappa is a better predictor of our models.


#### Accuracy, Precision and Recall

For our unoptimized models we have the following results:
```{r measures, echo=FALSE}
# Models
models <- c("Regular Regression", "Regular Decision Tree")

# Accuracy
train_accuracy <- c(cm_train_reg_model$overall['Accuracy'], cm_train_C50_model$overall['Accuracy'])
test_accuracy <- c(cm_test_reg_model$overall['Accuracy'], cm_test_C50_model$overall['Accuracy'])

# Precision
train_precision <- c(cm_train_reg_model$byClass['Precision'], cm_train_C50_model$byClass['Precision'])
test_precision <- c(cm_test_reg_model$byClass['Precision'], cm_test_C50_model$byClass['Precision'])

# Recall
train_recall <- c(cm_train_reg_model$byClass['Recall'], cm_train_C50_model$byClass['Recall'])
test_recall <- c(cm_test_reg_model$byClass['Recall'], cm_test_C50_model$byClass['Recall'])

# Create dataframe with results
measures_df <- data_frame(models, train_accuracy, test_accuracy, train_precision, test_precision, train_recall, test_recall)

# Show Dataframe
kable(measures_df, format="markdown")
```
Our models received pretty similar results, and bot of them are pretty good. In general the difference for all measures between the training and test set,
are basically identical too, which indicates that our models are probably not overfitting our data. We're going to take a further look in our data from the optimized models.



And for our optimized results, the one which use Lasso and Ridge (regression) and early stop (C5.0):
```{r measures_for_optimized, echo=FALSE}
# Models
models <- c("Optimized Regression", "Optimized Decision Tree")

# Accuracy
train_accuracy <- c(cm_train_reg_model_opt$overall['Accuracy'], cm_train_C50_model_opt$overall['Accuracy'])
test_accuracy <- c(cm_test_reg_model_opt$overall['Accuracy'], cm_test_C50_model_opt$overall['Accuracy'])

# Precision
train_precision <- c(cm_train_reg_model_opt$byClass['Precision'], cm_train_C50_model_opt$byClass['Precision'])
test_precision <- c(cm_test_reg_model_opt$byClass['Precision'], cm_test_C50_model_opt$byClass['Precision'])

# Recall
train_recall <- c(cm_train_reg_model_opt$byClass['Recall'], cm_train_C50_model_opt$byClass['Recall'])
test_recall <- c(cm_test_reg_model_opt$byClass['Recall'], cm_test_C50_model_opt$byClass['Recall'])

# Create dataframe with results
optimized_measures_df <- data_frame(models, train_accuracy, test_accuracy, train_precision, test_precision, train_recall, test_recall)

# Show Dataframe
kable(optimized_measures_df, format="markdown")
```

As we predicted the difference between them are negligible, which further confirm that we're not suffering from overfit.

#### Modelling Me

Let's use my own information to predict if I should have evaded or not.

```{r modeling_me}

# Get a random student from my semester
random_student <- train %>% filter(student_id == "8f47dc13a79411606b36240b73b53609")

my_grades <- random_student %>%
  mutate(evaded=FALSE, term="1" , vectorial=5.2 ,calculus=5.4 , ICS=5.9 , LP1=7.2 , RPT=8.6 , P1=7 , missing_records= 1)

my_grade <- my_grades %>% rowwise() %>%
  mutate(general_gpa = sum(vectorial, calculus, ICS, LP1, RPT, P1) / 6,
         compsci_gpa = sum(ICS, LP1, P1) / 3,
         math_gpa = sum(vectorial, calculus) / 2)

my_prediction <- predict(reg_model_opt, my_grades)

my_prediction
```

So our classifier would have classfied myself correctly.
































